{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5de52fe5-7e74-43bc-8b5e-a29f6c1dbaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f6f904-4bed-4b81-80a6-9aed9451034f",
   "metadata": {},
   "source": [
    "# Submission Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa205541-f00c-4970-9075-eb0e7a3ddfe5",
   "metadata": {},
   "source": [
    "We will use the following to convert csv files to the json format used for the evaluation.\n",
    "\n",
    "We first transform the test set (validation till test solutions are not released).\n",
    "\n",
    "For the evaluation soft labels, we use the mean of the annotator aggregations, with possible values (0, 1/3, 2/3, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d236e9a-672f-4122-af6c-d634c0d2de11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_dict_t1(stereo):\n",
    "    return {\"Stereotype\": stereo, \"NoStreotype\": 1 - stereo}\n",
    "\n",
    "\n",
    "def soft_dict_t2(row, stereo=\"stereotype\", imp=\"implicit\"):\n",
    "    implicit = row[stereo] * row[imp]\n",
    "    explicit = row[stereo] * (1 - row[imp])\n",
    "    return {\"Implicit\": implicit, \"Explicit\": explicit, \"NoStreotype\": 1 - row[stereo]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a296d7e5-2894-49a4-a92b-638b9edeeae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_to_json(test, name=\"test\"):\n",
    "    test[\"test_case\"] = \"DETESTS-Dis\"\n",
    "    # T1 HARD\n",
    "    df = test.copy()\n",
    "    df[\"value\"] = np.where(df[\"stereotype\"] == 1, \"Stereotype\", \"NoStereotype\")\n",
    "    df[[\"test_case\", \"id\", \"value\"]].to_json(f\"data/{name}_t1_hard.json\", orient=\"records\", indent=4)\n",
    "\n",
    "    # T2 HARD\n",
    "    df = test.copy()\n",
    "    df[\"value\"] = np.select(\n",
    "        [df[\"implicit\"] == 1, df[\"stereotype\"] == 1], [\"Implicit\", \"Explicit\"], default=\"NoStereotype\"\n",
    "    )\n",
    "    df[[\"test_case\", \"id\", \"value\"]].to_json(f\"data/{name}_t2_hard.json\", orient=\"records\", indent=4)\n",
    "\n",
    "    # T1 SOFT\n",
    "    df = test.copy()\n",
    "    df[\"stereotype_soft\"] = df[[\"stereotype_a1\", \"stereotype_a2\", \"stereotype_a3\"]].mean(axis=1)\n",
    "    df[\"value\"] = df[\"stereotype_soft\"].apply(soft_dict_t1)\n",
    "    df[[\"test_case\", \"id\", \"value\"]].to_json(f\"data/{name}_t1_soft.json\", orient=\"records\", indent=4)\n",
    "\n",
    "    # T2 SOFT\n",
    "    df = test.copy()\n",
    "    df[\"value\"] = df.apply(soft_dict_t2, args=(\"stereotype_soft\", \"implicit_soft\"), axis=1)\n",
    "    df[[\"test_case\", \"id\", \"value\"]].to_json(f\"data/{name}_t2_soft.json\", orient=\"records\", indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef9c4fa-53c5-4e4a-bd1c-1620124b5c46",
   "metadata": {},
   "source": [
    "We create a validation partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d2097da-f07e-422c-9d54-0f159be9d377",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "df = train\n",
    "train = df.sample(frac=0.8, random_state=42)\n",
    "validation = df.drop(train.index)\n",
    "\n",
    "train.to_csv(\"data/train_val.csv\", index=False)\n",
    "validation.to_csv(\"data/validation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1362b76-41e7-481d-b8ba-b94cf2e0dfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.read_csv(\"data/validation.csv\")\n",
    "val[\"stereotype_soft\"] = val[[\"stereotype_a1\", \"stereotype_a2\", \"stereotype_a3\"]].mean(axis=1)\n",
    "val[\"implicit_soft\"] = val[[\"implicit_a1\", \"implicit_a2\", \"implicit_a3\"]].mean(axis=1)\n",
    "\n",
    "test_to_json(val, \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aa7828a-2d90-47c5-93d1-a1580073fbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## When the test solutions are available\n",
    "# test = pd.read_csv(\"data/test_solutions.csv\")\n",
    "# test[\"stereotype_soft\"] = test[[\"stereotype_a1\", \"stereotype_a2\", \"stereotype_a3\"]].mean(axis=1)\n",
    "# test[\"implicit_soft\"] = test[[\"implicit_a1\", \"implicit_a2\", \"implicit_a3\"]].mean(axis=1)\n",
    "\n",
    "# test_to_json(test, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b596038-4e5a-4aec-8821-f2dcc5a4cf8c",
   "metadata": {},
   "source": [
    "The baselines may be converted to json as follows.\n",
    "\n",
    "You can use the same functions for your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c13e2d9-70bf-48f6-9d70-306effd1d97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_t1_hard(file):\n",
    "    df = pd.read_csv(file)\n",
    "    df[\"test_case\"] = \"DETESTS-Dis\"\n",
    "    df[\"value\"] = np.where(df[\"stereotype\"] == 1, \"Stereotype\", \"NoStereotype\")\n",
    "    df[[\"test_case\", \"id\", \"value\"]].to_json(file[:-4] + \".json\", orient=\"records\", indent=4)\n",
    "\n",
    "\n",
    "def json_t2_hard(file):\n",
    "    df = pd.read_csv(file)\n",
    "    df[\"test_case\"] = \"DETESTS-Dis\"\n",
    "    df[\"value\"] = np.select(\n",
    "        [df[\"implicit\"] == 1, df[\"stereotype\"] == 1], [\"Implicit\", \"Explicit\"], default=\"NoStereotype\"\n",
    "    )\n",
    "    df[[\"test_case\", \"id\", \"value\"]].to_json(file[:-4] + \".json\", orient=\"records\", indent=4)\n",
    "\n",
    "\n",
    "def json_t1_soft(file):\n",
    "    df = pd.read_csv(file)\n",
    "    df[\"test_case\"] = \"DETESTS-Dis\"\n",
    "    df[\"value\"] = df[\"stereotype\"].apply(soft_dict_t1)\n",
    "    df[[\"test_case\", \"id\", \"value\"]].to_json(file[:-4] + \".json\", orient=\"records\", indent=4)\n",
    "\n",
    "\n",
    "def json_t2_soft(file):\n",
    "    df = pd.read_csv(file)\n",
    "    df[\"test_case\"] = \"DETESTS-Dis\"\n",
    "    df[\"value\"] = df.apply(soft_dict_t2, axis=1)\n",
    "    df[[\"test_case\", \"id\", \"value\"]].to_json(file[:-4] + \".json\", orient=\"records\", indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "379b5c74-af14-4adc-92e4-55d2ed51fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob(\"baselines/validation/*_t1_hard.csv\"):\n",
    "    json_t1_hard(file)\n",
    "\n",
    "for file in glob.glob(\"baselines/validation/*_t2_hard.csv\"):\n",
    "    json_t2_hard(file)\n",
    "\n",
    "for file in glob.glob(\"baselines/validation/*_t1_soft.csv\"):\n",
    "    json_t1_soft(file)\n",
    "\n",
    "for file in glob.glob(\"baselines/validation/*_t2_soft.csv\"):\n",
    "    json_t2_soft(file)\n",
    "    \n",
    "for file in glob.glob(\"baselines/test/*_t1_hard.csv\"):\n",
    "    json_t1_hard(file)\n",
    "\n",
    "for file in glob.glob(\"baselines/test/*_t2_hard.csv\"):\n",
    "    json_t2_hard(file)\n",
    "\n",
    "for file in glob.glob(\"baselines/test/*_t1_soft.csv\"):\n",
    "    json_t1_soft(file)\n",
    "\n",
    "for file in glob.glob(\"baselines/test/*_t2_soft.csv\"):\n",
    "    json_t2_soft(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd0fa68-ff95-432c-8dda-cd0005d9a1a6",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "770961ea-8ebe-48a6-9385-33f0848e4b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import evaluate, main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073ce54f-fe00-41eb-abe5-ee97e5609e27",
   "metadata": {},
   "source": [
    "Here we provide an example of the metrics for the 4 tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "947fb4bf-afa3-44bf-a534-bb9a8beaf284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 17:45:10,551 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['FMeasure', 'Precision', 'Recall']\n",
      "2024-04-17 17:45:10,560 - pyevall.metrics.metrics - INFO -             evaluate() - Executing fmeasure evaluation method\n",
      "2024-04-17 17:45:10,562 - pyevall.metrics.metrics - INFO -             evaluate() - Executing precision evaluation method\n",
      "2024-04-17 17:45:10,562 - pyevall.metrics.metrics - INFO -             evaluate() - Executing recall evaluation method\n",
      "Task 1 Hard Labels {'F1': 0.8}\n",
      "2024-04-17 17:45:10,563 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['CrossEntropy']\n",
      "2024-04-17 17:45:10,567 - pyevall.metrics.metrics - INFO -             evaluate() - Executing Cross Entropy evaluation method\n",
      "Task 1 Soft Labels {'Cross Entropy': 1.5808285223439784}\n",
      "2024-04-17 17:45:10,568 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICMSoft', 'ICMSoftNorm']\n",
      "2024-04-17 17:45:10,573 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Soft evaluation method\n",
      "2024-04-17 17:45:10,576 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM-Soft Normalized evaluation method\n",
      "2024-04-17 17:45:10,576 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Soft evaluation method\n",
      "2024-04-17 17:45:10,579 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Soft evaluation method\n",
      "Task 2 Hard Labels {'ICM Soft': -10.576490286434076, 'ICM Soft Norm': 0}\n",
      "2024-04-17 17:45:10,582 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM', 'ICMNorm']\n",
      "2024-04-17 17:45:10,586 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n",
      "2024-04-17 17:45:10,587 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Normalized evaluation method\n",
      "2024-04-17 17:45:10,587 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n",
      "2024-04-17 17:45:10,588 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n",
      "Task 2 Soft Labels {'ICM': -2.9605947323337506e-16, 'ICM Norm': 0.49999999999999983}\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177da819-2ad2-413b-8628-772954c55875",
   "metadata": {},
   "source": [
    "You may try them with the baselines or your own models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6ac8c7d-b580-4818-a2f7-06150282c290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 17:45:11,903 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['FMeasure', 'Precision', 'Recall']\n",
      "2024-04-17 17:45:11,970 - pyevall.metrics.metrics - INFO -             evaluate() - Executing fmeasure evaluation method\n",
      "2024-04-17 17:45:12,220 - pyevall.metrics.metrics - INFO -             evaluate() - Executing precision evaluation method\n",
      "2024-04-17 17:45:12,221 - pyevall.metrics.metrics - INFO -             evaluate() - Executing recall evaluation method\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'F1': 0.27698185291308497}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = \"baselines/validation/tfidf_svc_t1_hard.json\"\n",
    "gold = \"data/validation_t1_hard.json\"\n",
    "evaluate(pred, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "278ad914-f4d0-4657-aabc-76f6189117c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-17 17:45:13,255 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM', 'ICMNorm']\n",
      "2024-04-17 17:45:13,322 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n",
      "2024-04-17 17:45:13,564 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Normalized evaluation method\n",
      "2024-04-17 17:45:13,564 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n",
      "2024-04-17 17:45:13,807 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ICM': 0.04972717204505462, 'ICM Norm': 0.5223741437225744}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = \"baselines/validation/tfidf_svc_t2_hard.json\"\n",
    "gold = \"data/validation_t2_hard.json\"\n",
    "evaluate(pred, gold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
